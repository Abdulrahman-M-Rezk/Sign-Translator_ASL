import cv2
import mediapipe as mp
import numpy as np
import tensorflow as tf
import json
import time

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª ---
MODEL_PATH = "best_htcn_model.keras"  # ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ø³Ù… Ù…Ù„Ù Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ØµØ­ÙŠØ­
MAP_PATH = "Models/best_htcn_model.keras/sign_to_prediction_index_map.json"

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ---
SEQ_LEN = 50       # Ø¹Ø¯Ø¯ Ø§Ù„ÙØ±ÙŠÙ…Ø§Øª (Buffer)
THRESHOLD = 0.6    # Ø£Ù‚Ù„ Ù†Ø³Ø¨Ø© Ø«Ù‚Ø© Ù„Ù‚Ø¨ÙˆÙ„ Ø§Ù„ÙƒÙ„Ù…Ø© (60%)

# --- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ (Mapping) ---
try:
    with open(MAP_PATH, 'r') as f:
        data = json.load(f)
        # Ù†Ø­ØªØ§Ø¬ Ø¹ÙƒØ³ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³: Ø§Ù„Ù…ÙØªØ§Ø­ ÙŠÙƒÙˆÙ† Ø§Ù„Ø±Ù‚Ù… ÙˆØ§Ù„Ù‚ÙŠÙ…Ø© ØªÙƒÙˆÙ† Ø§Ù„ÙƒÙ„Ù…Ø©
        # {0: "TV", 1: "after", ...}
        idx_to_word = {v: k for k, v in data.items()}
    print(f"âœ… Loaded {len(idx_to_word)} classes from JSON.")
except Exception as e:
    print(f"âŒ Error loading JSON: {e}")
    exit()

# --- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ---
try:
    print("ðŸ”„ Loading Model...")
    model = tf.keras.models.load_model(MODEL_PATH)
    print("âœ… Model Loaded Successfully!")
except Exception as e:
    print(f"âŒ Error loading Model. Make sure tensorflow is installed properly.\n{e}")
    exit()

# --- Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Normalization Logic) ---
# Ù†ÙØ³ Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø¯Ù‚Ø©
def get_hand_features(hand_landmarks, body_center):
    if hand_landmarks:
        hand_np = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])
        wrist = hand_np[0, :]
        local_hand = hand_np - wrist  # Hand relative to wrist
        wrist_context = wrist - body_center # Wrist relative to body
        return np.concatenate([local_hand.flatten(), wrist_context])
    else:
        return np.zeros(66) # (21*3 + 3)

def get_pose_features(pose_landmarks):
    if pose_landmarks:
        pose_np = np.array([[lm.x, lm.y, lm.z] for lm in pose_landmarks.landmark])
        # Ø­Ø³Ø§Ø¨ Ù…Ø±ÙƒØ² Ø§Ù„Ø¬Ø³Ù… (Ù…Ù†ØªØµÙ Ø§Ù„ÙƒØªÙÙŠÙ†)
        left_shoulder = pose_np[11, :]
        right_shoulder = pose_np[12, :]
        body_center = (left_shoulder + right_shoulder) / 2.0
        
        pose_centered = pose_np - body_center
        pose_xy = pose_centered[:, :2] # Ù†Ø£Ø®Ø° x, y ÙÙ‚Ø· Ù„Ù„Ø¬Ø³Ù…
        return pose_xy.flatten(), body_center
    else:
        return np.zeros(66), np.zeros(3)

# --- Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ---
mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils

cap = cv2.VideoCapture(0)
sequence = [] # Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù€ 50 ÙØ±ÙŠÙ…
current_word = "Waiting..."

with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret: break

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ÙŠØ¯ÙŠØ§ Ø¨Ø§ÙŠØ¨
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        image.flags.writeable = False
        results = holistic.process(image)
        image.flags.writeable = True
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

        # Ø±Ø³Ù… Ø§Ù„Ù†Ù‚Ø§Ø· (Ù„Ù„ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ ØªØ±Ø§Ùƒ)
        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)

        # --- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
        pose_vec, body_center = get_pose_features(results.pose_landmarks)
        lh_vec = get_hand_features(results.left_hand_landmarks, body_center)
        rh_vec = get_hand_features(results.right_hand_landmarks, body_center)

        # Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (ÙØ±ÙŠÙ… ÙˆØ§Ø­Ø¯)
        # Ø§Ù„ØªØ±ØªÙŠØ¨: Pose -> Left -> Right
        frame_features = np.concatenate([pose_vec, lh_vec, rh_vec])
        
        # Ø¥Ø¶Ø§ÙØªÙ‡ Ù„Ù„Ø°Ø§ÙƒØ±Ø©
        sequence.append(frame_features)
        sequence = sequence[-SEQ_LEN:] # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¢Ø®Ø± 50 ÙÙ‚Ø·

        # --- Ø§Ù„ØªÙˆÙ‚Ø¹ (Inference) ---
        if len(sequence) == SEQ_LEN:
            # ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¯Ø§ØªØ§ (Batch Dimension)
            input_data = np.expand_dims(sequence, axis=0)
            
            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
            start_t = time.time()
            res = model.predict(input_data, verbose=0)[0]
            latency = (time.time() - start_t) * 1000
            
            predicted_idx = np.argmax(res)
            confidence = res[predicted_idx]

            if confidence > THRESHOLD:
                word = idx_to_word[predicted_idx]
                current_word = f"{word} ({confidence:.0%})"
                color = (0, 255, 0) # Ø£Ø®Ø¶Ø±
            else:
                current_word = "..."
                color = (0, 0, 255) # Ø£Ø­Ù…Ø±

            # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø´Ø©
            cv2.putText(image, current_word, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
            cv2.putText(image, f"Lat: {latency:.1f}ms", (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)

        cv2.imshow('Sign Language Project - Real Time', image)

        if cv2.waitKey(10) & 0xFF == ord('q'):
            break

cap.release()
cv2.destroyAllWindows()