import os
# Ø®ÙÙŠ Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„ØªÙ†Ø³Ø±ÙÙ„Ùˆ Ø§Ù„Ù…Ø²Ø¹Ø¬Ø©
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' 

import tensorflow as tf
import mediapipe as mp   
import cv2
import numpy as np
import json
import time

# ================= CONFIG (Ø¹Ø¯Ù„ Ù‡Ù†Ø§) =================
# 1. Ù…Ø³Ø§Ø± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù„ÙŠ Ø¯Ø±Ø¨ØªÙ‡ (ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø§Ø³Ù… ÙˆØ§Ù„Ù…ÙƒØ§Ù†)
MODEL_PATH = '/mnt/Hub_1/Mix/Projects/Graduation -Project/Notebooks/best_bilstm_local.keras' # Ø£Ùˆ Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯ .h5 Ø­Ø³Ø¨ Ù…Ø§ Ø­ÙØ¸ØªÙ‡

# 2. Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ù‚Ø§Ù…ÙˆØ³
LABEL_MAP_PATH = r'/mnt/Hub_1/Mix/Projects/Graduation -Project/Data/label_map.json'

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø«Ø§Ø¨ØªØ© (Ø²ÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ù„Ø¸Ø¨Ø·)
SEQUENCE_LENGTH = 50
CONF_THRESH = 0.5

# ================= Load Resources =================
print("â³ Loading Model & Resources...")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
try:
    model = tf.keras.models.load_model(MODEL_PATH)
    print("âœ… Model Loaded Successfully!")
except Exception as e:
    print(f"âŒ Error loading model: {e}")
    exit()

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ (ÙˆØ¹ÙƒØ³Ù‡ Ø¹Ø´Ø§Ù† Ù†Ø­ÙˆÙ„ Ø§Ù„Ø±Ù‚Ù… Ù„Ø§Ø³Ù…)
with open(LABEL_MAP_PATH, 'r') as f:
    label_map = json.load(f)
# Ø¹ÙƒØ³ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³: {0: 'apple', 1: 'book', ...}
inv_label_map = {v: k for k, v in label_map.items()}
print(f"âœ… Label Map Loaded ({len(label_map)} classes).")

# ================= Helper Functions (Preprocessing) =================
# Ù†ÙØ³ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù„ÙŠ Ø§Ø³ØªØ®Ø¯Ù…Ù†Ø§Ù‡Ø§ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ù„Ø¸Ø¨Ø· (Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹ ØªÙƒÙˆÙ† Ù…ØªØ·Ø§Ø¨Ù‚Ø©)
mp_holistic = mp.solutions.holistic

def normalize_hand(pts):
    ref = pts[0].copy()
    scale = np.linalg.norm(pts[9] - ref)
    if scale < 1e-6: scale = 1.0
    return (pts - ref) / scale

def choose_best_hands(multi_hand_landmarks, multi_handedness):
    chosen = {}
    if multi_hand_landmarks is None or multi_handedness is None:
        return chosen
    for lm, hd in zip(multi_hand_landmarks, multi_handedness):
        label = hd.classification[0].label.upper()
        conf  = float(hd.classification[0].score)
        if conf < CONF_THRESH: continue
        pts = np.array([[p.x, p.y, p.z] for p in lm.landmark], dtype=np.float32)
        chosen[label] = pts
    return chosen

def compute_torso_center_and_scale(pose_landmarks):
    torso_center = np.array([0.5, 0.5], dtype=np.float32)
    torso_scale = 1.0
    try:
        ps = pose_landmarks
        def get_xy(idx):
            lm = ps.landmark[idx]
            return np.array([lm.x, lm.y], dtype=np.float32)
        
        left_sh, right_sh = get_xy(11), get_xy(12)
        left_hip, right_hip = get_xy(23), get_xy(24)
        
        shoulder_center = (left_sh + right_sh) / 2.0
        hip_center = (left_hip + right_hip) / 2.0
        torso_center = (shoulder_center + hip_center) / 2.0
        
        shoulder_dist = np.linalg.norm(left_sh - right_sh)
        hip_dist = np.linalg.norm(left_hip - right_hip)
        torso_scale = max(shoulder_dist, hip_dist, 1e-6)
    except: pass
    return torso_center, float(torso_scale)

def extract_features(results):
    # 198 Feature Vector Extraction
    feat = np.zeros(198, dtype=np.float32)
    
    torso_center = np.array([0.5, 0.5], dtype=np.float32)
    torso_scale = 1.0

    if results.pose_landmarks:
        torso_center, torso_scale = compute_torso_center_and_scale(results.pose_landmarks)
        pose_xy = np.array([[lm.x, lm.y] for lm in results.pose_landmarks.landmark], dtype=np.float32)
        pose_norm = (pose_xy - torso_center[None, :]) / torso_scale
        feat[0:66] = pose_norm.flatten()

    chosen = choose_best_hands(getattr(results, 'multi_hand_landmarks', None),
                               getattr(results, 'multi_handedness', None))

    # Left Hand
    if 'LEFT' in chosen:
        left_pts = chosen['LEFT']
        feat[66:129] = normalize_hand(left_pts)[:, :3].flatten()
        wrist = left_pts[0]
        wrist_rel = np.array([(wrist[0] - torso_center[0]) / torso_scale,
                              (wrist[1] - torso_center[1]) / torso_scale,
                              wrist[2] / max(torso_scale, 1e-6)], dtype=np.float32)
        feat[129:132] = wrist_rel

    # Right Hand
    if 'RIGHT' in chosen:
        right_pts = chosen['RIGHT']
        feat[132:195] = normalize_hand(right_pts)[:, :3].flatten()
        wrist = right_pts[0]
        wrist_rel = np.array([(wrist[0] - torso_center[0]) / torso_scale,
                              (wrist[1] - torso_center[1]) / torso_scale,
                              wrist[2] / max(torso_scale, 1e-6)], dtype=np.float32)
        feat[195:198] = wrist_rel
        
    return feat

# ================= Main Loop =================
print("ğŸ¥ Opening Camera...")
cap = cv2.VideoCapture(0) # 0 Ù„Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©ØŒ 1 Ù„Ùˆ Ø¹Ù†Ø¯Ùƒ ÙƒØ§Ù…ÙŠØ±Ø§ ØªØ§Ù†ÙŠØ©

# Ù…Ø®Ø²Ù† Ù„Ù„ÙØ±ÙŠÙ…Ø§Øª (Sliding Window)
sequence = [] 
current_word = "Waiting..."
probability = 0.0

with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret: break

        # 1. ØªØ¬Ù‡ÙŠØ² Ø§Ù„ØµÙˆØ±Ø©
        image = cv2.flip(frame, 1) # Ù…Ø±Ø§ÙŠØ©
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Ù‚Ø§Ø·
        results = holistic.process(image_rgb)
        
        # 3. Ø±Ø³Ù… Ø§Ù„Ù†Ù‚Ø§Ø· Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø´Ø© (Ù„Ù„ØªÙˆØ¶ÙŠØ­)
        mp.solutions.drawing_utils.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)
        mp.solutions.drawing_utils.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
        mp.solutions.drawing_utils.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)

        # 4. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
        keypoints = extract_features(results)
        sequence.append(keypoints)
        
        # Ù†Ø­ØªÙØ¸ Ø¨Ø¢Ø®Ø± 50 ÙØ±ÙŠÙ… Ø¨Ø³
        sequence = sequence[-SEQUENCE_LENGTH:]

        # 5. Ø§Ù„ØªÙˆÙ‚Ø¹ (Ù„Ù…Ø§ Ù†Ø¬Ù…Ø¹ 50 ÙØ±ÙŠÙ…)
        if len(sequence) == SEQUENCE_LENGTH:
            # ØªØ­ÙˆÙŠÙ„ Ù„Ø´ÙƒÙ„ ÙŠÙ‚Ø¨Ù„Ù‡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ (1, 50, 198)
            input_data = np.expand_dims(sequence, axis=0)
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤
            res = model.predict(input_data, verbose=0)[0]
            
            # Ù†Ø§Ø®Ø¯ Ø£Ø¹Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„
            best_idx = np.argmax(res)
            probability = res[best_idx]
            
            # Ù„Ùˆ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ Ø¹Ø§Ù„ÙŠ ÙƒÙØ§ÙŠØ©ØŒ Ù†Ø¹Ø±Ø¶ Ø§Ù„ÙƒÙ„Ù…Ø©
            if probability > 0.6: # Ù…Ù…ÙƒÙ† ØªØºÙŠØ± Ø§Ù„Ù€ Threshold Ø¯Ù‡
                current_word = inv_label_map[best_idx]
            else:
                current_word = "..."

        # 6. Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø´Ø©
        # Ù…Ø³ØªØ·ÙŠÙ„ Ø®Ù„ÙÙŠØ© Ù„Ù„ÙƒÙ„Ø§Ù…
        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)
        # Ø§Ù„ÙƒÙ„Ù…Ø©
        cv2.putText(image, f"{current_word} ({probability*100:.1f}%)", (10,30), 
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)
        
        # Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù… (Ø¨ÙŠÙˆØ±ÙŠÙƒ Ø¬Ù…Ø¹Øª ÙƒØ§Ù… ÙØ±ÙŠÙ… Ù…Ù† Ø§Ù„Ù€ 50)
        progress = len(sequence) / SEQUENCE_LENGTH
        cv2.rectangle(image, (0, 40), (int(640 * progress), 45), (0, 255, 0), -1)

        cv2.imshow('Sign Language Translator (BiLSTM)', image)

        # Ø§Ù„Ø®Ø±ÙˆØ¬ Ø¨Ø²Ø± 'q'
        if cv2.waitKey(10) & 0xFF == ord('q'):
            break

cap.release()
cv2.destroyAllWindows()